{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "import pickle\n",
    "import sklearn\n",
    "import string\n",
    "import wordcloud\n",
    "import seaborn as sns\n",
    "import operator\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"development.jsonl\"\n",
    "TEST_PATH = \"evaluation.jsonl\"\n",
    "SUBMISSION = \"submission.csv\"\n",
    "\n",
    "class Tweet:\n",
    "    def __init__(self, uid, text, retweet_count, favorite_count, is_positive=None):\n",
    "        self.uid = str(uid)\n",
    "        self.text = str(text)\n",
    "        self.retweet_count = int(retweet_count)\n",
    "        self.favorite_count = int(favorite_count)\n",
    "        if is_positive is not None:\n",
    "            self.is_positive = bool(is_positive)\n",
    "        else:\n",
    "            self.is_positive = None\n",
    "    def __str__(self):\n",
    "        return \"{}\".format(self.text)\n",
    "    \n",
    "# considerando ad esempio attributo \"is_quoted\"\n",
    "#class Tweet:\n",
    "#    def __init__(self, uid, text, retweet_count, favorite_count, is_quoted, is_positive=None):\n",
    "#        self.uid = str(uid)\n",
    "#        self.text = str(text)\n",
    "#        self.retweet_count = int(retweet_count)\n",
    "#        self.favorite_count = int(favorite_count)\n",
    "#        self.is_quoted = str(is_quoted)\n",
    "#        if is_positive is not None:\n",
    "#            self.is_positive = bool(is_positive)\n",
    "#        else:\n",
    "#            self.is_positive = None\n",
    "#    def __str__(self):\n",
    "#        return \"{}\".format(self.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_measures = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\", \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\", \"second\", \"seconds\", \"minute\", \"minutes\", \"hour\", \"hours\", \"day\", \"days\", \"week\", \"weeks\", \"month\", \"months\", \"year\", \"years\", \"decade\", \"decades\", \"century\", \"centuries\"]\n",
    "\n",
    "verbs = [\"i’ve\", \"i’m\", \"you’ve\", \"you’re\", \"he’s\", \"she’s\", \"it’s\", \"we’ve\", \"we’re\", \"they’re\", \"they’ve\", \"i’d\", \"you’d\", \"he’d\", \"she’d\", \"it’d\", \"we’d\", \"they’d\", \"i’ll\", \"you’ll\", \"he’ll\", \"she’ll\", \"it’ll\", \"we’ll\", \"they’ll\", \"don’t\", \"doesn’t\", \"didn’t\", \"i've\", \"i'm\", \"you've\", \"you're\", \"he's\", \"she's\", \"it's\", \"we've\", \"we're\", \"they're\", \"they've\", \"i'd\", \"you'd\", \"he'd\", \"she'd\", \"it'd\", \"we'd\", \"they'd\", \"i'll\", \"you'll\", \"he'll\", \"she'll\", \"it'll\", \"we'll\", \"they'll\", \"don't\", \"doesn't\", \"didn't\", \"I'm\", \"I've\", \"I'd\", \"I’m\", \"I’ve\", \"I’d\", \"don’t\"]\n",
    "\n",
    "punctuations = [\"!\", \"%\", \"&\", '\"', \"(\", \")\", \"*\", \"+\", \",\", \".\", \"...\", \"/\", \":\", \";\", \"<\", \"=\", \"?\", \"[\", \"]\", \"^\", \"_\", \"`\", \"{\", \"|\", \"}\", \"~\"]\n",
    "\n",
    "stopwords = [\"ourselves\", \"hers\", \"between\", \"with\", \"again\", \"yourself\", \"but\", \"there\", \"about\", \"once\", \"during\", \"out\", \"very\", \"having\", \"they\", \"own\", \"an\", \"be\", \"some\", \"for\", \"do\", \"its\", \"yours\", \"such\", \"into\", \"of\", \"most\", \"itself\", \"other\", \"off\", \"is\", \"s\", \"am\", \"or\", \"who\", \"as\", \"from\", \"him\", \"each\", \"the\", \"themselves\", \"until\", \"below\", \"are\", \"we\", \"this\", \"these\", \"your\", \"his\", \"through\", \"don\", \"nor\", \"me\", \"were\", \"her\", \"more\", \"himself\", \"down\", \"should\", \"our\", \"their\", \"while\", \"above\", \"both\", \"up\", \"to\", \"ours\", \"had\", \"she\", \"when\", \"at\", \"any\", \"before\", \"them\", \"same\", \"and\", \"been\", \"have\", \"in\", \"will\", \"on\", \"does\", \"yourselves\", \"then\", \"that\", \"because\", \"what\", \"over\", \"why\", \"so\", \"can\", \"did\", \"now\", \"under\", \"he\", \"you\", \"herself\", \"has\", \"just\", \"where\", \"too\", \"only\", \"myself\", \"which\", \"those\", \"i\", \"after\", \"few\", \"whom\", \"t\", \"being\", \"if\", \"theirs\", \"my\", \"a\", \"by\", \"doing\", \"it\", \"how\", \"further\", \"was\", \"here\", \"than\"]\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    # convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # punctuation\n",
    "    tweet = tweet.replace(\"\\n\", \"\")\n",
    "    tweet = tweet.replace(\"\\t\", \"\")\n",
    "    \n",
    "    for pun in punctuations:\n",
    "        tweet = tweet.replace(pun, \" \")\n",
    "    \n",
    "    tweet = tweet.replace(\"-\", \"\")\n",
    "    tweet = tweet.replace(\"\\u2026\", \" \")\n",
    "    \n",
    "    # remove time measures\n",
    "    for times in time_measures:\n",
    "        tweet = tweet.replace(times, \"\")\n",
    "        \n",
    "    # remove conjugate verbs\n",
    "    for verb in verbs:\n",
    "        new_tweet = []\n",
    "        for token in tweet.split(\" \"):\n",
    "            if token != verb.lower():\n",
    "                new_tweet.append(token)\n",
    "        tweet = \" \".join(new_tweet)\n",
    "    \n",
    "    # remove stopwords\n",
    "    #for word in nltk.corpus.stopwords.words(\"english\"):\n",
    "    for word in stopwords:\n",
    "        new_tweet = []\n",
    "        for token in tweet.split(\" \"):\n",
    "            if token != word.lower():\n",
    "                new_tweet.append(token)\n",
    "        tweet = \" \".join(new_tweet)\n",
    "    \n",
    "    new_tweet = []\n",
    "    for token in tweet.split(\" \"):\n",
    "        if not token.startswith(\"http\") and not token.startswith(\"https\") and \"$\" not in token and not token.isdigit() and not \"am\" in token and not \"pm\" in token:\n",
    "            #new_tweet.append(token)\n",
    "            new_tweet.append(stemmer.stem(token))\n",
    "    tweet = \" \".join(new_tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "# not token.startswith(\"@\") and\n",
    "#and not token.startswith(\"#\")\n",
    "#and not token == \"rt\"\n",
    "\n",
    "def load_tweets(filename, train):\n",
    "    result = {}\n",
    "    with open(filename, \"r\") as fp:\n",
    "        counter = 0\n",
    "        for row in fp:\n",
    "            tweet = None\n",
    "            json_tweet = json.loads(row)\n",
    "            if train:\n",
    "                tweet = Tweet(uid=json_tweet[\"id_str\"], text=clean_tweet(json_tweet[\"full_text\"]), retweet_count=json_tweet[\"retweet_count\"], favorite_count=json_tweet[\"favorite_count\"], is_positive=json_tweet[\"class\"])\n",
    "                result[tweet.uid] = tweet\n",
    "            else:\n",
    "                tweet = Tweet(uid=json_tweet[\"id_str\"], text=clean_tweet(json_tweet[\"full_text\"]), retweet_count=json_tweet[\"retweet_count\"], favorite_count=json_tweet[\"favorite_count\"])\n",
    "                result[counter] = tweet\n",
    "            counter += 1\n",
    "    return result\n",
    "\n",
    "#if train:\n",
    "#    tweet = Tweet(uid=json_tweet[\"id_str\"], text=clean_tweet(json_tweet[\"full_text\"]), retweet_count=json_tweet[\"retweet_count\"], favorite_count=json_tweet[\"favorite_count\"], is_quoted=json_tweet[\"quoted_status_id_str\"], is_positive=json_tweet[\"class\"])\n",
    "#    result[tweet.uid] = tweet\n",
    "#else:\n",
    "#    tweet = Tweet(uid=json_tweet[\"id_str\"], text=clean_tweet(json_tweet[\"full_text\"]), retweet_count=json_tweet[\"retweet_count\"], favorite_count=json_tweet[\"favorite_count\"], is_quoted=json_tweet[\"quoted_status_id_str\"])\n",
    "#    result[counter] = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_tweets = load_tweets(TRAIN_PATH, True)\n",
    "test_tweets = load_tweets(TEST_PATH, False)\n",
    "\n",
    "labels = []\n",
    "processed_features = []\n",
    "processed_test_features = []\n",
    "positive_tweets = []\n",
    "negative_tweets = []\n",
    "negative = 0\n",
    "positive = 0\n",
    "\n",
    "for key in all_tweets:\n",
    "    if all_tweets[key].is_positive is True:\n",
    "        labels.append(1)\n",
    "        positive_tweets.append(all_tweets[key].text)\n",
    "        positive += 1\n",
    "    else:\n",
    "        labels.append(0)\n",
    "        negative_tweets.append(all_tweets[key].text)\n",
    "        negative += 1\n",
    "    #string = all_tweets[key].text\n",
    "    #if all_tweets[key].is_quoted != \"None\":\n",
    "    #    string = all_tweets[key].text+\" quoted\"\n",
    "    #processed_features.append(string)\n",
    "    processed_features.append(all_tweets[key].text)\n",
    "\n",
    "for key in test_tweets:\n",
    "    #string = test_tweets[key].text\n",
    "    #if test_tweets[key].is_quoted != \"None\":\n",
    "    #    string = test_tweets[key].text+\" quoted\"\n",
    "    #processed_test_features.append(string)\n",
    "    processed_test_features.append(test_tweets[key].text)\n",
    "\n",
    "print(len(all_tweets))\n",
    "print(len(test_tweets))\n",
    "print(positive)\n",
    "print(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentuali = [pos_percentuale, neg_percentuale]\n",
    "valori = [positive, negative]\n",
    "lab = \"Positive class\", \"Negative class\"\n",
    "plt.title(\"Label distribution\")\n",
    "plt.pie(valori, labels=lab, autopct='%1.2f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#processed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_occurrencies = {}\n",
    "\n",
    "class WordFreq:\n",
    "\n",
    "    def __init__(self, word, number_occ, positive, negative):\n",
    "        self.word = word\n",
    "        self.number_occ = number_occ\n",
    "        self.positive = positive\n",
    "        self.negative = negative\n",
    "\n",
    "for key in all_tweets:\n",
    "    for word in all_tweets[key].text.split():\n",
    "        if word not in word_occurrencies:\n",
    "            if all_tweets[key].is_positive:\n",
    "                word_occurrencies[word] = WordFreq(word, 1, 1, 0)\n",
    "            else:\n",
    "                word_occurrencies[word] = WordFreq(word, 1, 0, 1)\n",
    "        else:\n",
    "            word_occurrencies[word].number_occ += 1\n",
    "            if all_tweets[key].is_positive:\n",
    "                word_occurrencies[word].positive += 1\n",
    "            else:\n",
    "                word_occurrencies[word].negative += 1\n",
    "\n",
    "number_occ = 20\n",
    "#number_occ = 50\n",
    "counter = 0\n",
    "words = []\n",
    "top_N_count = []\n",
    "top_N_positive = []\n",
    "top_N_negative = []\n",
    "word_occurrencies = sorted(word_occurrencies.values(), key=operator.attrgetter(\"number_occ\"), reverse=True)\n",
    "for wo in word_occurrencies:\n",
    "    if counter <= number_occ:\n",
    "        if counter != 0:\n",
    "            print(\"Word {} Number {} Positive {} Negative {}\".format(wo.word, wo.number_occ, wo.positive, wo.negative))\n",
    "    words.append(wo.word)\n",
    "    top_N_count.append(wo.number_occ)\n",
    "    top_N_positive.append(wo.positive)\n",
    "    top_N_negative.append(wo.negative)\n",
    "    counter += 1\n",
    "\n",
    "#tot_50_words = words[0:50]\n",
    "#top_50_pos = top_N_positive[0:50]\n",
    "#top_50_neg = top_N_negative[0:50]\n",
    "#for i in range(50):\n",
    "#    print(f\"{tot_50_words[i]}, {top_50_pos[i]}, {top_50_neg[i]}\")\n",
    "\n",
    "tot_words = words[0:20]\n",
    "top_N_positive = top_N_positive[0:20]\n",
    "top_N_negative = top_N_negative[0:20]\n",
    "\n",
    "# Top 20 words\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('TOP 20 WORDS', fontsize=18, pad=25.0, alpha=0.85, weight='normal')\n",
    "x = np.arange(20)\n",
    "width = 0.25\n",
    "plt.bar(x-(width/2), (top_N_positive), width, linewidth=0.5, label='Positive class')\n",
    "plt.bar(x+(width/2), (top_N_negative), width, linewidth=0.5, label='Negative class')\n",
    "plt.legend()\n",
    "\n",
    "#plt.bar(x-(width/2), (top_N_positive), width, color='#00800050', edgecolor='#00800095', linewidth=0.5, label='Positive class')\n",
    "#plt.bar(x+(width/2), (top_N_negative), width, color='#80000050', edgecolor='#80000095', linewidth=0.5, label='Negative class')\n",
    "plt.grid(alpha=0.2)\n",
    "plt.xticks(x, tot_words, rotation=45, fontsize=12)\n",
    "plt.ylabel('Occurrency', alpha=0.85, weight='normal', fontsize=14, labelpad=18.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_occurrencies = {}\n",
    "\n",
    "class WordFreqDiv:\n",
    "\n",
    "    def __init__(self, word, number_occ):\n",
    "        self.word = word\n",
    "        self.number_occ = number_occ\n",
    "\n",
    "for key in all_tweets:\n",
    "    if all_tweets[key].is_positive:\n",
    "        for word in all_tweets[key].text.split():\n",
    "            if word not in word_occurrencies:\n",
    "                word_occurrencies[word] = WordFreqDiv(word, 1)\n",
    "            else:\n",
    "                word_occurrencies[word].number_occ += 1\n",
    "\n",
    "# Positive\n",
    "number_occ = 20\n",
    "counter = 0\n",
    "positive_words = []\n",
    "top_N_positive_count = []\n",
    "word_occurrencies = sorted(word_occurrencies.values(), key=operator.attrgetter(\"number_occ\"), reverse=True)\n",
    "for wo in word_occurrencies:\n",
    "    if counter < number_occ:\n",
    "        print(\"Word {} Number {}\".format(wo.word, wo.number_occ))\n",
    "    positive_words.append(wo.word)\n",
    "    top_N_positive_count.append(wo.number_occ)\n",
    "    counter += 1\n",
    "\n",
    "# Generate positive wordcloud\n",
    "serialized_positive_tweets = \" \".join(positive_tweets)\n",
    "wordcloud = wordcloud.WordCloud(width=1200, height=800, background_color=\"white\").generate(serialized_positive_tweets)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_occurrencies = {}\n",
    "\n",
    "for key in all_tweets:\n",
    "    if not all_tweets[key].is_positive:\n",
    "        for word in all_tweets[key].text.split():\n",
    "            if word not in word_occurrencies:\n",
    "                word_occurrencies[word] = WordFreqDiv(word, 1)\n",
    "            else:\n",
    "                word_occurrencies[word].number_occ += 1\n",
    "\n",
    "# Negative\n",
    "number_occ = 20\n",
    "counter = 0\n",
    "negative_words = []\n",
    "top_N_negative_count = []\n",
    "word_occurrencies = sorted(word_occurrencies.values(), key=operator.attrgetter(\"number_occ\"), reverse=True)\n",
    "for wo in word_occurrencies:\n",
    "    if counter < number_occ:\n",
    "        print(\"Word {} Number {}\".format(wo.word, wo.number_occ))\n",
    "    negative_words.append(wo.word)\n",
    "    top_N_negative_count.append(wo.number_occ)\n",
    "    counter += 1\n",
    "\n",
    "import wordcloud\n",
    "# Generate negative wordcloud\n",
    "serialized_negative_tweets = \" \".join(negative_tweets)\n",
    "wordcloud = wordcloud.WordCloud(width=1200, height=800, background_color=\"black\").generate(serialized_negative_tweets)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot Top 20 positive words\n",
    "positive_words = positive_words[0:20]\n",
    "top_N_positive_count = top_N_positive_count[0:20]\n",
    "x = np.arange(20)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('TOP 20 POSITIVE WORDS', fontsize=18, pad=25.0, alpha=0.85, weight='normal')\n",
    "plt.grid(alpha=0.2)\n",
    "plt.bar(x, width=0.8, height=top_N_positive_count, label=\"Positive class\")\n",
    "plt.legend()\n",
    "plt.xticks(x, positive_words, rotation=45, fontsize=12)\n",
    "plt.ylabel('Occurrency', alpha=0.85, weight='normal', fontsize=14, labelpad=18.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot Top 20 negative words\n",
    "negative_words = negative_words[0:20]\n",
    "top_N_negative_count = top_N_negative_count[0:20]\n",
    "x = np.arange(20)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('TOP 20 NEGATIVE WORDS', fontsize=18, pad=25.0, alpha=0.85, weight='normal')\n",
    "plt.grid(alpha=0.2)\n",
    "plt.bar(x, width=0.8, height=top_N_negative_count)\n",
    "plt.bar(x, width=0.8, height=top_N_negative_count, label=\"Negative class\")\n",
    "plt.legend()\n",
    "plt.xticks(x, negative_words, rotation=45, fontsize=12)\n",
    "plt.ylabel('Occurrency', alpha=0.85, weight='normal', fontsize=14, labelpad=18.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# VECTORIZER\n",
    "\n",
    "LOWER_BOUND = 1\n",
    "UPPER_BOUND = 3\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(LOWER_BOUND, UPPER_BOUND))\n",
    "#vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(LOWER_BOUND, UPPER_BOUND), max_features=250000)\n",
    "vectorizer.fit(processed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY DIFFERENT CLASSIFIERS\n",
    "\"\"\"\n",
    "ESTIMATORS = 100\n",
    "KERNEL = \"linear\"\n",
    "NEIGHBORS = 5\n",
    "\n",
    "models = []\n",
    "models_name = [\"RandomForestClassifier\", \"LogisticRegression\", \"KNeighborsClassifier\", \"SVC\", \"MultinomialNB\"]\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=-1, n_estimators=ESTIMATORS, random_state=0)\n",
    "models.append(model)\n",
    "model = LogisticRegression(n_jobs=-1, random_state=0)\n",
    "models.append(model)\n",
    "model = KNeighborsClassifier(n_neighbors=NEIGHBORS, n_jobs=-1)\n",
    "models.append(model)\n",
    "model = SVC(kernel=KERNEL)\n",
    "models.append(model)\n",
    "model = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "models.append(model)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TRY DIFFERENT CLASSIFIERS\n",
    "\"\"\"\n",
    "best_accuracy = 0\n",
    "best_i = -1\n",
    "for i in range(0, 5):\n",
    "    print(\"Training model {}\".format(models_name[i]))\n",
    "    my_pipeline = make_pipeline(vectorizer, models[i])\n",
    "    accuracy_cv = cross_val_score(my_pipeline, processed_features, labels, cv=10, scoring=\"accuracy\")\n",
    "    mean = accuracy_cv.mean()\n",
    "    std = accuracy_cv.std() ** 2\n",
    "    print(f\"Accuracy (statistics): {mean:.6f} (+/- {std:.6f})\")\n",
    "    model_accuracy = mean\n",
    "    if model_accuracy > best_accuracy:\n",
    "        best_accuracy = model_accuracy\n",
    "        print(\"Best model {}\".format(models_name[i]))\n",
    "        best_i = i\n",
    "model = models[best_i]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNING HYPERPARAMETERS BEST MODEL (LOGISTICREGRESSION)\n",
    "\n",
    "\"\"\"cs = [0.5, 1, 1.5, 2, 2.5]\n",
    "penalties = [\"l1\", \"l2\"]\n",
    "solvers = [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\n",
    "\n",
    "for c in cs:\n",
    "    for penalty in penalties:\n",
    "        for solver in solvers:\n",
    "            if (solver == \"newton-cg\" or solver == \"lbfgs\" or solver == \"sag\") and penalty not in [\"l2\", \"none\"]:\n",
    "                continue\n",
    "            print(\"\\nC: {} Penality: {} Solver: {}\".format(c, penalty, solver))\n",
    "            model = LogisticRegression(solver=solver, penalty=penalty, C=c, n_jobs=-1, random_state=0)\n",
    "            my_pipeline = make_pipeline(vectorizer, model)\n",
    "            predictions = cross_val_predict(my_pipeline, processed_features, labels, cv=10)\n",
    "            print(sklearn.metrics.classification_report(labels, predictions))\n",
    "            print(\"Precision score: {}\".format(sklearn.metrics.precision_score(labels, predictions)))\n",
    "            print(\"Accuracy score: {}\".format(sklearn.metrics.accuracy_score(labels, predictions)))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"cs = [0.5, 1, 1.5, 2, 2.5]\n",
    "penalties = [\"elasticnet\", \"none\"]\n",
    "solvers = [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\n",
    "\n",
    "for c in cs:\n",
    "    for penalty in penalties:\n",
    "        for solver in solvers:\n",
    "            if (solver == \"newton-cg\" or solver == \"lbfgs\" or solver == \"sag\") and penalty not in [\"l2\", \"none\"]:\n",
    "                continue\n",
    "            if penalty == \"elasticnet\" and solver != \"saga\":\n",
    "                continue\n",
    "            if penalty == \"none\" and solver == \"liblinear\":\n",
    "                continue\n",
    "            print(\"C: {} Penality: {} Solver: {}\".format(c, penalty, solver))\n",
    "            if penalty == \"elasticnet\":\n",
    "                # l1_ratio 0.5, so combination of penalty \"l1\" and \"l2\"\n",
    "                # l1_ratio=0 is equivalent to set penalty=\"l2\"\n",
    "                # l1_ratio=1 is equivalent to set penalty=\"l1\"\n",
    "                model = LogisticRegression(solver=solver, penalty=penalty, C=c, l1_ratio=0.5, n_jobs=-1, random_state=0)\n",
    "                my_pipeline = make_pipeline(vectorizer, model)\n",
    "            else:\n",
    "                model = LogisticRegression(solver=solver, penalty=penalty, C=c, n_jobs=-1, random_state=0)\n",
    "                my_pipeline = make_pipeline(vectorizer, model)\n",
    "            predictions = cross_val_predict(my_pipeline, processed_features, labels, cv=10)\n",
    "            print(sklearn.metrics.classification_report(labels, predictions))\n",
    "            print(\"Precision score: {}\".format(sklearn.metrics.precision_score(labels, predictions)))\n",
    "            print(\"Accuracy score: {}\".format(sklearn.metrics.accuracy_score(labels, predictions)))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST HYPERPARAMETERS FOUND\n",
    "solver = \"lbfgs\"\n",
    "penalty = \"l2\"\n",
    "c = 1.5\n",
    "\n",
    "model = LogisticRegression(solver=solver, penalty=penalty, C=c, n_jobs=-1, random_state=0)\n",
    "#model = LinearSVC(loss=\"hinge\", C=2.0, random_state=0)\n",
    "my_pipeline = make_pipeline(vectorizer, model)\n",
    "\n",
    "accuracy_cv = cross_val_score(my_pipeline, processed_features, labels, cv=10, scoring=\"accuracy\")\n",
    "print(\"Score accuracy: {}\".format(accuracy_cv))\n",
    "print(\"Mean accuracy: {}\".format(accuracy_cv.mean()))\n",
    "mean = accuracy_cv.mean()\n",
    "std = accuracy_cv.std() ** 2\n",
    "print(f\"Accuracy (statistics): {mean:.3f} (+/- {std:.6f})\")\n",
    "\n",
    "# predictions (training)\n",
    "predictions = cross_val_predict(my_pipeline, processed_features, labels, cv=10)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = sklearn.metrics.confusion_matrix(labels, predictions)\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax=ax, cmap=\"GnBu\", fmt='g')\n",
    "ax.set_title(\"Confusion Matrix\")\n",
    "\n",
    "print(sklearn.metrics.classification_report(labels, predictions))\n",
    "print(\"Precision score: {}\".format(sklearn.metrics.precision_score(labels, predictions)))\n",
    "print(\"Accuracy score: {}\".format(sklearn.metrics.accuracy_score(labels, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions (test)\n",
    "my_pipeline.fit(processed_features, labels)\n",
    "predicted_values = my_pipeline.predict(processed_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "header = \"Id,Predicted\"\n",
    "first = True\n",
    "counter = 0\n",
    "\n",
    "with open(SUBMISSION, \"w\") as file_out:\n",
    "    for predicted in predicted_values:\n",
    "        if first:\n",
    "            file_out.write(\"{}\\n{},{}\\n\".format(header, counter, predicted))\n",
    "            first = False\n",
    "        else:\n",
    "            file_out.write(\"{},{}\\n\".format(counter, predicted))\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
